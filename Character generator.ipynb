{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from __future__ import print_function\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from model import TextGenerator \n",
    "from get_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preparing raw data\n",
    "filename = \"alice.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "seq_len = 50\n",
    "n_chars, n_vocab, n_examples, X, Y, int_to_char, char_to_int = get_data(raw_text, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (30) : unknown error at /home/ilya/pytorch/torch/lib/THC/THCGeneral.c:66",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-38e951cb2404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_vocab\u001b[0m \u001b[0;31m# vocabulary size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model2.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda2/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda2/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda2/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mroot_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 deserialized_objects[root_key] = restore_location(\n\u001b[0;32m--> 348\u001b[0;31m                     data_type(size), location)\n\u001b[0m\u001b[1;32m    349\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda2/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda2/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mdevice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda2/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m     84\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (30) : unknown error at /home/ilya/pytorch/torch/lib/THC/THCGeneral.c:66"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "H = 256 #hidden size\n",
    "D = n_vocab # features\n",
    "layers = 2 # number of lsmt layers\n",
    "B = 8 # batch size\n",
    "V = n_vocab # vocabulary size\n",
    "model = TextGenerator(D,H,layers,B,V)\n",
    "model.load_state_dict(torch.load('model2.txt'))\n",
    "model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9, nesterov = True)\n",
    "print (model)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Variable containing:\n",
      " 0.7401\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 100 Variable containing:\n",
      " 0.7913\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 200 Variable containing:\n",
      " 0.8375\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 300 Variable containing:\n",
      " 0.7926\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 400 Variable containing:\n",
      " 0.8023\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 500 Variable containing:\n",
      " 0.7674\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 600 Variable containing:\n",
      " 0.8653\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 700 Variable containing:\n",
      " 0.7263\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 800 Variable containing:\n",
      " 0.8492\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 900 Variable containing:\n",
      " 0.7971\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 1000 Variable containing:\n",
      " 0.8031\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 1100 Variable containing:\n",
      " 0.7566\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 1200 Variable containing:\n",
      " 0.8038\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 1300 Variable containing:\n",
      " 0.8964\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 1400 Variable containing:\n",
      " 0.8648\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 1500 Variable containing:\n",
      " 0.8299\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 1600 Variable containing:\n",
      " 0.7954\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 1700 Variable containing:\n",
      " 0.8406\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 1800 Variable containing:\n",
      " 0.7801\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 1900 Variable containing:\n",
      " 0.7635\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 2000 Variable containing:\n",
      " 0.7658\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 2100 Variable containing:\n",
      " 0.8656\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 2200 Variable containing:\n",
      " 0.8293\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 2300 Variable containing:\n",
      " 0.8385\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 2400 Variable containing:\n",
      " 0.7779\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 2500 Variable containing:\n",
      " 0.8536\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 2600 Variable containing:\n",
      " 0.9397\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 2700 Variable containing:\n",
      " 0.7499\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 2800 Variable containing:\n",
      " 0.8960\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 2900 Variable containing:\n",
      " 0.8516\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 3000 Variable containing:\n",
      " 0.6925\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 3100 Variable containing:\n",
      " 0.9639\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 3200 Variable containing:\n",
      " 0.8724\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 3300 Variable containing:\n",
      " 0.7991\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 3400 Variable containing:\n",
      " 0.8454\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 3500 Variable containing:\n",
      " 0.7124\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 3600 Variable containing:\n",
      " 0.8121\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 3700 Variable containing:\n",
      " 0.6915\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 3800 Variable containing:\n",
      " 0.7241\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 3900 Variable containing:\n",
      " 0.7763\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 4000 Variable containing:\n",
      " 0.8171\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 4100 Variable containing:\n",
      " 0.8635\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 4200 Variable containing:\n",
      " 0.8224\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 4300 Variable containing:\n",
      " 0.8261\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 4400 Variable containing:\n",
      " 0.8297\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 4500 Variable containing:\n",
      " 0.7680\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 4600 Variable containing:\n",
      " 0.7922\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 4700 Variable containing:\n",
      " 0.8351\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 4800 Variable containing:\n",
      " 0.8412\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 4900 Variable containing:\n",
      " 0.9030\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 5000 Variable containing:\n",
      " 0.8382\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 5100 Variable containing:\n",
      " 0.8718\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 5200 Variable containing:\n",
      " 0.7517\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 5300 Variable containing:\n",
      " 0.7169\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 5400 Variable containing:\n",
      " 0.9140\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 5500 Variable containing:\n",
      " 0.8421\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 5600 Variable containing:\n",
      " 0.8143\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 5700 Variable containing:\n",
      " 0.9654\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 5800 Variable containing:\n",
      " 0.7214\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 5900 Variable containing:\n",
      " 0.8870\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 6000 Variable containing:\n",
      " 0.7879\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 6100 Variable containing:\n",
      " 0.8473\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 6200 Variable containing:\n",
      " 0.7689\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 6300 Variable containing:\n",
      " 0.9032\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 6400 Variable containing:\n",
      " 0.7918\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 6500 Variable containing:\n",
      " 0.7491\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 6600 Variable containing:\n",
      " 0.9222\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 6700 Variable containing:\n",
      " 0.8965\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 6800 Variable containing:\n",
      " 0.8840\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 6900 Variable containing:\n",
      " 0.8868\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 7000 Variable containing:\n",
      " 0.8316\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 7100 Variable containing:\n",
      " 0.7781\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 7200 Variable containing:\n",
      " 0.7131\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 7300 Variable containing:\n",
      " 0.8293\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 7400 Variable containing:\n",
      " 0.9459\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 7500 Variable containing:\n",
      " 0.7546\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 7600 Variable containing:\n",
      " 0.7407\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 7700 Variable containing:\n",
      " 0.7549\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 7800 Variable containing:\n",
      " 0.8167\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 7900 Variable containing:\n",
      " 0.6991\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 8000 Variable containing:\n",
      " 0.7684\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 8100 Variable containing:\n",
      " 0.8978\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 8200 Variable containing:\n",
      " 0.8339\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 8300 Variable containing:\n",
      " 0.9069\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 8400 Variable containing:\n",
      " 0.7830\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 8500 Variable containing:\n",
      " 0.8169\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 8600 Variable containing:\n",
      " 0.8229\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 8700 Variable containing:\n",
      " 0.8051\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 8800 Variable containing:\n",
      " 0.7242\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 8900 Variable containing:\n",
      " 0.8846\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 9000 Variable containing:\n",
      " 0.9200\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 9100 Variable containing:\n",
      " 0.7553\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 9200 Variable containing:\n",
      " 0.8046\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 9300 Variable containing:\n",
      " 0.7639\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 9400 Variable containing:\n",
      " 0.8009\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 9500 Variable containing:\n",
      " 0.7765\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 9600 Variable containing:\n",
      " 0.8697\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 9700 Variable containing:\n",
      " 0.7211\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 9800 Variable containing:\n",
      " 0.8109\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 9900 Variable containing:\n",
      " 0.7435\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 10000 Variable containing:\n",
      " 0.7792\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 10100 Variable containing:\n",
      " 0.8050\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 10200 Variable containing:\n",
      " 0.8365\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 10300 Variable containing:\n",
      " 0.9375\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 10400 Variable containing:\n",
      " 0.7289\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 10500 Variable containing:\n",
      " 0.7794\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 10600 Variable containing:\n",
      " 0.7465\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 10700 Variable containing:\n",
      " 0.6310\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 10800 Variable containing:\n",
      " 0.8490\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 10900 Variable containing:\n",
      " 0.8331\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 11000 Variable containing:\n",
      " 0.8272\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 11100 Variable containing:\n",
      " 0.8863\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 11200 Variable containing:\n",
      " 0.7783\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 11300 Variable containing:\n",
      " 0.7441\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 11400 Variable containing:\n",
      " 0.7331\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 11500 Variable containing:\n",
      " 0.7784\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 11600 Variable containing:\n",
      " 0.8114\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 11700 Variable containing:\n",
      " 0.8360\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 11800 Variable containing:\n",
      " 0.7593\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 11900 Variable containing:\n",
      " 0.9070\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 12000 Variable containing:\n",
      " 0.7931\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 12100 Variable containing:\n",
      " 0.9016\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 12200 Variable containing:\n",
      " 0.7361\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 12300 Variable containing:\n",
      " 0.8375\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 12400 Variable containing:\n",
      " 0.7892\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 12500 Variable containing:\n",
      " 0.8137\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 12600 Variable containing:\n",
      " 0.8084\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 12700 Variable containing:\n",
      " 0.8485\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 12800 Variable containing:\n",
      " 0.7636\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 12900 Variable containing:\n",
      " 0.8509\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 13000 Variable containing:\n",
      " 0.7268\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 13100 Variable containing:\n",
      " 0.7642\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 13200 Variable containing:\n",
      " 0.7991\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 13300 Variable containing:\n",
      " 0.7364\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 13400 Variable containing:\n",
      " 0.8016\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 13500 Variable containing:\n",
      " 0.7345\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 13600 Variable containing:\n",
      " 0.8471\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 13700 Variable containing:\n",
      " 0.8486\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 13800 Variable containing:\n",
      " 0.7394\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 13900 Variable containing:\n",
      " 0.7725\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 14000 Variable containing:\n",
      " 0.8321\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 14100 Variable containing:\n",
      " 0.7907\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 14200 Variable containing:\n",
      " 0.9129\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 14300 Variable containing:\n",
      " 0.7724\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 14400 Variable containing:\n",
      " 0.6988\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 14500 Variable containing:\n",
      " 0.7366\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 14600 Variable containing:\n",
      " 0.8204\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 14700 Variable containing:\n",
      " 0.6966\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 14800 Variable containing:\n",
      " 0.7395\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 14900 Variable containing:\n",
      " 0.7435\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 15000 Variable containing:\n",
      " 0.7754\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 15100 Variable containing:\n",
      " 0.7902\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 15200 Variable containing:\n",
      " 0.8358\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 15300 Variable containing:\n",
      " 0.8615\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 15400 Variable containing:\n",
      " 0.8083\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 15500 Variable containing:\n",
      " 0.8691\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 15600 Variable containing:\n",
      " 0.9091\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 15700 Variable containing:\n",
      " 0.7333\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 15800 Variable containing:\n",
      " 0.7579\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 15900 Variable containing:\n",
      " 0.7184\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 16000 Variable containing:\n",
      " 0.8406\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 16100 Variable containing:\n",
      " 0.7911\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 16200 Variable containing:\n",
      " 0.8074\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 16300 Variable containing:\n",
      " 0.8274\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 16400 Variable containing:\n",
      " 0.7163\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 16500 Variable containing:\n",
      " 0.7567\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 16600 Variable containing:\n",
      " 0.9020\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 16700 Variable containing:\n",
      " 0.8179\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 16800 Variable containing:\n",
      " 0.8468\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 16900 Variable containing:\n",
      " 0.8749\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 17000 Variable containing:\n",
      " 0.8571\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 17100 Variable containing:\n",
      " 0.8307\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 17200 Variable containing:\n",
      " 0.7891\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 17300 Variable containing:\n",
      " 0.7890\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 17400 Variable containing:\n",
      " 0.7646\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 17500 Variable containing:\n",
      " 0.8630\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 17600 Variable containing:\n",
      " 0.8692\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 17700 Variable containing:\n",
      " 0.7786\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 17800 Variable containing:\n",
      " 0.7238\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 17900 Variable containing:\n",
      " 0.8238\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 18000 Variable containing:\n",
      " 0.8553\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 18100 Variable containing:\n",
      " 0.8151\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 18200 Variable containing:\n",
      " 0.8052\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 18300 Variable containing:\n",
      " 0.8535\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 18400 Variable containing:\n",
      " 0.7913\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 18500 Variable containing:\n",
      " 0.8556\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 18600 Variable containing:\n",
      " 0.7279\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "0 18700 Variable containing:\n",
      " 0.8265\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_iter = n_examples/B\n",
    "\n",
    "for epoch in range(1):\n",
    "    hidden = model.init_hidden(B)\n",
    "    for it in xrange(num_iter):\n",
    "        model.zero_grad()\n",
    "\n",
    "        train = []\n",
    "        target = []\n",
    "        \n",
    "        inp = prepeare_X(X[it:it+B],B,seq_len, n_vocab)\n",
    "        inp = inp.cuda() \n",
    "        \n",
    "        target = Y[it:it+B]\n",
    "        tensor1 = torch.LongTensor(target)\n",
    "        m = autograd.Variable(tensor1)\n",
    "        m = m.cuda()\n",
    "        \n",
    "        tag_scores, hidden = model(inp, hidden)\n",
    "        tag_scores = tag_scores.view(B*seq_len, -1)\n",
    "        m = m.view(B*seq_len)\n",
    "        loss = loss_function(tag_scores, m)\n",
    "        if ( it%100 == 0 ):         \n",
    "            print (epoch, it , loss)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        hidden[0].detach_()\n",
    "        hidden[1].detach_()\n",
    "        loss.backward(retain_variables=True)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n",
      "-10000\n"
     ]
    }
   ],
   "source": [
    "start = np.random.randint(0, len(X))\n",
    "pattern = X[start]\n",
    "s = \"\"\n",
    "hidden = model.init_hidden(1)\n",
    "for it in xrange(1000):\n",
    "\n",
    "    inp = prepeare_X(pattern,1,seq_len, n_vocab)\n",
    "    scores, hidden = model.forward2(inp.cuda(), hidden) #for GPU\n",
    "    hidden[0].detach_()\n",
    "    hidden[1].detach_()\n",
    "#    scores, hidden = model.forward2(inp, hidden)\n",
    "    qwer, tmp = torch.max(scores,1)\n",
    "    final = np.squeeze(tmp.data.cpu().numpy()) #for GPU\n",
    "#    final = np.squeeze(tmp.data.numpy())\n",
    "    predicted_char = int_to_char[final[seq_len-1]]\n",
    "    pattern.append(final[seq_len-1])\n",
    "    pattern = pattern[1:]\n",
    "    s += predicted_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at was on the top of it.\n",
      "\n",
      "‘come, it’s very curious,’ said the caterpillar.\n",
      "\n",
      "‘well, i’ve seen them so often, of course,’ the dormouse fell as she could, ‘it’ll be sending me on messages next!’ and she began thinking over other head down of the mock turtle.\n",
      "\n",
      "‘and how don’t believe there’s an atom of meaning in it,’ said the king, ‘and don’t be all day about it, you know--’\n",
      "\n",
      "‘i don’t know it was your table,’ said alice.\n",
      "\n",
      "‘why, she,’ said the caterpillar.\n",
      "\n",
      "‘well, i’ve seen them so often, of course,’ the dormouse fell as she could, ‘it’ll be sending me on messages next!’ and she began thinking over other head down of the mock turtle.\n",
      "\n",
      "‘and how don’t believe there’s an atom of meaning in it,’ said the king, ‘and don’t be all day about it, you know--’\n",
      "\n",
      "‘i don’t know it was your table,’ said alice.\n",
      "\n",
      "‘why, she,’ said the caterpillar.\n",
      "\n",
      "‘well, i’ve seen them so often, of course,’ the dormouse fell as she could, �\n"
     ]
    }
   ],
   "source": [
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ModelFile = open('model2.txt', 'w')\n",
    "torch.save(model.state_dict(), ModelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
